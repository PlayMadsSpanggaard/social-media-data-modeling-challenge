{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQGstO1f8bMD"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import csv\n",
        "\n",
        "# Initialize the Reddit API client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='8YMAaXsl11ShtMeshdQNyg',\n",
        "    client_secret='wj5FJDoJqIgXZdsfhXXoYLscah3yAw',\n",
        "    user_agent='Nancy Amandi'\n",
        ")\n",
        "\n",
        "# Define the output CSV file\n",
        "output_file = '/content/reddit_skibidi_toilet_comments.csv'\n",
        "\n",
        "# Define the header for the CSV file\n",
        "header = ['post_id', 'subreddit', 'created_utc', 'selftext', 'post_url', 'post_title',\n",
        "          'link_flair_text', 'score', 'num_comments', 'upvote_ratio',\n",
        "          'comment_id', 'comment_author', 'comment_body', 'comment_score', 'comment_created_utc']\n",
        "\n",
        "# Create the CSV file and write the header\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=header)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Search for posts related to \"Skibidi Toilet\"\n",
        "    subreddit = reddit.subreddit(\"all\")\n",
        "    search_results = subreddit.search(\"Skibidi Toilet\", sort='comments', time_filter='all')\n",
        "\n",
        "    # Extract relevant details from each post and its comments\n",
        "    for post in search_results:\n",
        "        print(f\"Processing post: {post.id} - {post.title}\")  # Debug: Print post info\n",
        "\n",
        "        try:\n",
        "            post.comments.replace_more(limit=None)  # To get all comments\n",
        "\n",
        "            # Iterate over top-level comments\n",
        "            for comment in post.comments.list():\n",
        "                post_details = {\n",
        "                    'post_id': post.id,\n",
        "                    'subreddit': str(post.subreddit),\n",
        "                    'created_utc': post.created_utc,\n",
        "                    'selftext': post.selftext,\n",
        "                    'post_url': post.url,\n",
        "                    'post_title': post.title,\n",
        "                    'link_flair_text': post.link_flair_text,\n",
        "                    'score': post.score,\n",
        "                    'num_comments': post.num_comments,\n",
        "                    'upvote_ratio': post.upvote_ratio,\n",
        "                    'comment_id': comment.id,\n",
        "                    'comment_author': str(comment.author),\n",
        "                    'comment_body': comment.body,\n",
        "                    'comment_score': comment.score,\n",
        "                    'comment_created_utc': comment.created_utc\n",
        "                }\n",
        "\n",
        "                # Write the post details to the CSV file\n",
        "                writer.writerow(post_details)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing post: {post.id} - {e}\")\n",
        "\n",
        "print(f\"Data extraction complete. Data saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "Qz0s2JkfuaA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load data into a DataFrame\n",
        "df = pd.read_csv('/content/reddit_skibidi_toilet_comments.csv')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to analyze sentiment\n",
        "def analyze_sentiment(comment):\n",
        "    sentiment = analyzer.polarity_scores(comment)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Apply sentiment analysis to the comments column\n",
        "df['sentiment_score'] = df['comment_body'].apply(analyze_sentiment)\n",
        "\n",
        "# Select only the necessary columns for the output\n",
        "sentiment_df = df[['post_id', 'comment_id', 'sentiment_score']]\n",
        "\n",
        "# Write the sentiment analysis results to a CSV file\n",
        "sentiment_df.to_csv('/content/sentiment_scores.csv', index=False)\n",
        "print(\"Sentiment scores written to 'sentiment_scores.csv'.\")\n",
        "\n",
        "# Now, find the most popular terms in the comments\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the comments to get the term frequencies\n",
        "term_matrix = vectorizer.fit_transform(df['comment_body'].fillna(''))\n",
        "\n",
        "# Sum the occurrences of each term\n",
        "term_frequencies = term_matrix.sum(axis=0)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "term_counts = [(terms[i], term_frequencies[0, i]) for i in range(len(terms))]\n",
        "\n",
        "# Sort terms by frequency\n",
        "term_counts_sorted = sorted(term_counts, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Convert the term counts to a DataFrame\n",
        "terms_df = pd.DataFrame(term_counts_sorted, columns=['term', 'count'])\n",
        "\n",
        "# Write the term counts to a CSV file\n",
        "terms_df.to_csv('/content/term_counts.csv', index=False)\n",
        "print(\"Term counts written to 'term_counts.csv'.\")"
      ],
      "metadata": {
        "id": "R6gCSyOWvJN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv('/content/reddit_skibidi_toilet_comments.csv')\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the 'selftext' column to get term frequencies\n",
        "term_matrix = vectorizer.fit_transform(df['selftext'].fillna(''))\n",
        "\n",
        "# Sum the occurrences of each term\n",
        "term_frequencies = term_matrix.sum(axis=0)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "term_counts = [(terms[i], term_frequencies[0, i]) for i in range(len(terms))]\n",
        "\n",
        "# Sort terms by frequency\n",
        "term_counts_sorted = sorted(term_counts, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Convert the term counts to a DataFrame\n",
        "terms_df = pd.DataFrame(term_counts_sorted, columns=['term', 'count'])\n",
        "\n",
        "# Write the term counts to a CSV file\n",
        "terms_df.to_csv('/content/terms_from_posts.csv', index=False)\n",
        "print(\"Term counts from posts written to 'terms_from_posts.csv'.\")"
      ],
      "metadata": {
        "id": "8IOnJchfwg1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAzRqwNj4RF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}